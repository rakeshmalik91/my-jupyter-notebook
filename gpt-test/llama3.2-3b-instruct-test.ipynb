{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "707a94ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install transformers tiktoken blobfile llama-stack\n",
    "# !pip install google-api-python-client google-auth google-auth-oauthlib google-auth-httplib2\n",
    "# !pip install torch fairscale fire sentencepiece protobuf pyyaml"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6089f035",
   "metadata": {},
   "source": [
    "*get LLAMA_REPO_URL from https://www.llama.com/llama-downloads/*\n",
    "\n",
    "- $env:PATH += \";%USERPROFILE%\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.13_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python313\\Scripts\"\n",
    "- $env:LLAMA_REPO_URL = \"https://llama3-2-lightweight.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoieDAwZXVhc3h2MmN6cm1jZXZ2aGhkYmN5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTItbGlnaHR3ZWlnaHQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTc1NTUxMzgyNX19fV19&Signature=n9xX1DRBt-hhv5oe2nqi0ldRmFPAMEV3g0UFUvufLXRed28Jj-38f%7EUV%7EnThjxGGKklDLkOt7VxS-9Kh02nYjk0wMWldD0tKmnxj8XBATG13Vt4s%7E41WfpUEMv1rJtvb81KqwgVUdetoNwOD1dUYfQPhUrvV2L8TvKqwyPat0Paz0lnSma1evhhSEltkJD15e6VQcHpf6Gbq454lqjx2NbtVxj1Wv0hjl4V5U2kC1S7nt8QSFLtZ6rB-9nNUM-yEZKWYym12PNwXrTbP6omRTr4nCHX22xVgZFGCm8aNtZ5V6IxETWiDhKdf3srReGefbH6sXan8BY5oEivNn9J4yw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1486291695705908\"\n",
    "- llama model list\n",
    "- llama model download --source meta --model-id Llama3.2-3B-Instruct --meta-url $env:LLAMA_REPO_URL\n",
    "- llama model verify-download --model-id Llama3.2-3B-Instruct\n",
    "\n",
    "- cd D:\\Projects\\my-jupyter-notebook\\gpt-test\n",
    "- mv \"$env:USERPROFILE\\.llama\\checkpoints\\Llama3.2-3B-Instruct\" \"llama-models/\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e56c74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converting the tokenizer.\n",
      "Saving a LlamaTokenizerFast to llama-models/Llama3.2-3B-Instruct-transformed.\n",
      "Converting the model.\n",
      "Fetching all parameters from the checkpoint at llama-models/Llama3.2-3B-Instruct.\n",
      "Loading the checkpoint in a Llama model.\n",
      "Saving in the Transformers format.\n",
      "Saving to disk.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Loading checkpoint shards:   0%|          | 0/29 [00:00<?, ?it/s]\n",
      "Loading checkpoint shards:  41%|####1     | 12/29 [00:00<00:00, 107.74it/s]\n",
      "Loading checkpoint shards:  79%|#######9  | 23/29 [00:00<00:00, 97.98it/s] \n",
      "Loading checkpoint shards: 100%|##########| 29/29 [00:00<00:00, 92.67it/s]\n"
     ]
    }
   ],
   "source": [
    "# !python llama-models/convert_llama_weights_to_hf.py --input_dir llama-models/Llama3.2-3B-Instruct --model_size 3B --llama_version 3.2 --output_dir llama-models/Llama3.2-3B-Instruct-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "833b09f0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a25913e523e147b79e7b91d2b60c0ee6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import LlamaForCausalLM, AutoTokenizer\n",
    "import torch\n",
    "import time\n",
    "\n",
    "import os\n",
    "os.chdir(\"D:/Projects/my-jupyter-notebook/gpt-test\")\n",
    "\n",
    "model_name = \"Llama3.2-3B-Instruct\"\n",
    "transformed_model_path = f\"llama-models/{model_name}-transformed\"\n",
    "model = LlamaForCausalLM.from_pretrained(transformed_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformed_model_path)\n",
    "\n",
    "def chat(model, device, prompt, max_new_tokens, silent=False):\n",
    "    print(f\"Prompt: {prompt}\") if not silent else None\n",
    "    model = model.to(device)\n",
    "    start_time = time.time()\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_new_tokens=max_new_tokens, eos_token_id=None)\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"Response ({elapsed_time:.4f}s on {model.device}) : {response[0]}\\n{'-'*20}\\n\") if not silent else None\n",
    "    return response[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2acf1955-f525-492f-9a64-b0b72c31cca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me an interesting fact about elephants\n",
      "Response (4.1522s on cuda:0) : Tell me an interesting fact about elephants\n",
      "I'd love to learn something new about these magnificent creatures. Here's a fascinating fact:\n",
      "\n",
      "**Elephants have a highly developed brain structure, and their brain-to-body mass ratio is similar to that of humans.** In fact, studies have shown that the cerebral cortex of an elephant is similar in structure and organization to that of a human brain, with areas dedicated to processing sensory information, memory, and social behavior. This suggests that elephants may possess a level\n",
      "--------------------\n",
      "\n",
      "Prompt: Tell me an interesting fact about elephants\n",
      "Response (37.2174s on cpu) : Tell me an interesting fact about elephants\n",
      "Elephants have a highly developed sense of empathy and compassion. They have been known to show signs of grief and mourning when they lose a family member, and they will often go to great lengths to care for and comfort each other in times of need. In fact, elephants have been observed displaying behaviors such as:\n",
      "* Touching trunks and embracing each other to comfort each other\n",
      "* Venerating the dead by touching their trunks to the body of a\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "response = chat(model, torch.device(\"cuda\"), prompt, 100)\n",
    "response = chat(model, torch.device(\"cpu\"), prompt, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58031e5c-4901-40e5-88f2-d2ffa0ebd88d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me an interesting fact about elephants\n",
      "Response (22.3962s on cuda:0) : Tell me an interesting fact about elephants\n",
      "Here's one: Did you know that elephants have a highly developed brain and are considered one of the smartest animals on Earth? In fact, their brain-to-body mass ratio is similar to that of humans, which is one of the highest among all land animals. This means that elephants have a large amount of brain power relative to their body size, allowing them to exhibit complex behaviors, problem-solving abilities, and even empathy and self-awareness. In fact, studies have shown that elephants are able to recognize themselves in mirrors, a cognitive ability that is rare in the animal kingdom. Isn't that cool? \n",
      "Here's another interesting fact about elephants: \n",
      "Elephants have a highly developed sense of smell, with a trunk that contains millions of olfactory receptors, which is more sensitive than a bloodhound's nose. This allows them to detect subtle changes in their environment, including the scent of distant water sources or the presence of predators. In fact, elephants have been known to use their sense of smell to find food that is buried underground, and to detect the scent of their family members after they have been separated. \n",
      "Elephants are also able to communicate with each other through a wide range of vocalizations, including rumbles, roars, and trumpets. These vocalizations can be heard for miles and are used to convey a range of emotions and intentions, from joy and excitement to fear and warning. In fact, elephants are able to recognize and respond to the unique vocalizations of their family members, allowing them to maintain close bonds and social relationships. \n",
      "Elephants are also highly social animals, living in large matriarchal herds that are led by the oldest female. These herds are known for their complex social dynamics and cooperation, with individuals working together to protect their young, find food, and care for each other. In fact, elephants have been observed showing empathy and compassion towards each other, including comforting a distressed family member or helping a struggling individual to get back on its feet. \n",
      "Elephants are also known for their impressive physical strength and endurance, with adult males weighing up to 15,000 pounds (6,800 kg) and reaching speeds of up to 15 miles per hour (24 km/h). They are also able to move long distances with ease, with some elephants traveling over 12,000 miles (19,312 km) in a single year. \n",
      "Elephants have a unique way of communicating through\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "response = chat(model, torch.device(\"cuda\"), prompt, 500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "0ab981c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me an interesting fact about elephants\n",
      "Response (258.4570s on cuda:0) : Tell me an interesting fact about elephants\n",
      "I'd be happy to! Here's an interesting fact about elephants:\n",
      "\n",
      "**Elephants have a highly developed brain and can recognize themselves in a mirror!**\n",
      "\n",
      "In fact, studies have shown that elephants are one of the few animals, along with chimpanzees and dolphins, that possess a level of self-awareness, known as mirror self-recognition. This means that they can look at themselves in a mirror and recognize their own reflection. This cognitive ability is often considered a hallmark of human intelligence, and it's fascinating to think that elephants, with their massive size and complex social behavior, possess this level of self-awareness.\n",
      "\n",
      "Would you like to know more about elephants or is there something else you'd like to explore?\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "response = chat(model, torch.device(\"cuda\"), prompt, 3000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "647aef95",
   "metadata": {},
   "source": [
    "# Text to Speech"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b44e9b86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pyttsx3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8d82b5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyttsx3\n",
    "\n",
    "def say(response, rate):\n",
    "    engine = pyttsx3.init()\n",
    "    engine.setProperty(\"rate\", rate)\n",
    "    engine.setProperty(\"volume\", 1.0)\n",
    "    engine.say(response)\n",
    "    engine.runAndWait()\n",
    "    pyttsx3.engine.Engine.stop(engine)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1591f7e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Tell me an interesting fact about elephants\n",
      "Response (4.3249s on cuda:0) : Tell me an interesting fact about elephants\n",
      "Here's one: Elephants have a highly developed sense of empathy and can recognize and respond to the emotions of other elephants. They have been known to show compassion and kindness to each other, often going out of their way to comfort a distressed or grieving elephant. In fact, studies have shown that elephants are more empathetic towards each other than humans are towards each other. Isn't that fascinating? \n",
      "What do you think about this fact? Do you find\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "say(chat(model, torch.device(\"cuda\"), prompt, max_new_tokens=100), rate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14614db2",
   "metadata": {},
   "source": [
    "# Speech to Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5554952e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install SpeechRecognition pyaudio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8c28408",
   "metadata": {},
   "outputs": [],
   "source": [
    "import speech_recognition as sr\n",
    "\n",
    "def listen(phrase_time_limit):\n",
    "    recognizer = sr.Recognizer()\n",
    "    with sr.Microphone() as source:\n",
    "        print(\"Say something...\")\n",
    "        audio = recognizer.listen(source, phrase_time_limit=phrase_time_limit)\n",
    "    try:\n",
    "        text = recognizer.recognize_google(audio)\n",
    "        return text\n",
    "    except sr.UnknownValueError:\n",
    "        print(\"Sorry, could not understand audio\")\n",
    "    except sr.RequestError:\n",
    "        print(\"API unavailable or no internet\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c627dda",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "Prompt: hello how r u\n",
      "Response (4.3159s on cuda:0) : hello how r u today\n",
      "I'm doing great, thanks for asking! Just got back from a nice walk and enjoying the sunshine. How about you? How's your day going so far?\n",
      "\n",
      "(By the way, I noticed you used the informal \"r u\" instead of \"are you\". That's totally fine in informal conversations, but just a heads up for those who might not be used to it!) \n",
      "\n",
      "So, what's on your mind today? Want to chat about something in particular\n",
      "--------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "say(chat(model, torch.device(\"cuda\"), listen(), max_new_tokens=100), rate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c400db1",
   "metadata": {},
   "source": [
    "# Chatbot loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2c2a126",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "\n",
      "Prompt: hello how r u\n",
      "Response (1.2610s on cuda:0) : hello how r u today?\n",
      "I'm doing alright, thanks for asking! Just got back from a walk outside and enjoying the sunshine. How about\n",
      "--------------------\n",
      "\n",
      "Say something...\n",
      "\n",
      "Prompt: I am fine\n",
      "Response (1.4429s on cuda:0) : I am fine, but my friend is experiencing a bit of a crisis. I want to be supportive, but I don't know how to help\n",
      "--------------------\n",
      "\n",
      "Say something...\n",
      "\n",
      "Prompt: who is he\n",
      "Response (1.4085s on cuda:0) : who is he\n",
      "I think you meant to ask \"Who is he?\" but didn't specify who \"he\" is. Could you please provide\n",
      "--------------------\n",
      "\n",
      "Say something...\n",
      "\n",
      "Prompt: I am in your friend\n",
      "Response (1.2259s on cuda:0) : I am in your friend's house, and you are the host, and you've invited me over for a casual dinner party. The evening is\n",
      "--------------------\n",
      "\n",
      "Say something...\n",
      "\n",
      "Prompt: how is the weather today\n",
      "Response (1.2677s on cuda:0) : how is the weather today in different parts of the world\n",
      "The weather can vary greatly depending on the location and time of year. Here's a\n",
      "--------------------\n",
      "\n",
      "Say something...\n",
      "\n",
      "Exiting chatbot loop.\n"
     ]
    }
   ],
   "source": [
    "while True:\n",
    "    prompt = listen()\n",
    "    if not prompt:\n",
    "        say(\"No input detected, please try again.\", rate=150)\n",
    "        continue\n",
    "    if prompt.lower().strip() in [\"exit\", \"quit\", \"goodbye\", \"bye\", \"good bye\"]:\n",
    "        say(\"Good bye!\", rate=150)\n",
    "        print(\"Exiting chatbot loop.\")\n",
    "        break\n",
    "    response = chat(model, torch.device(\"cuda\"), prompt, max_new_tokens=30)\n",
    "    say(response, rate=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72fa70c3",
   "metadata": {},
   "source": [
    "# Chatbot loop with context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e435e15c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Say something...\n",
      "User said: how are you\n",
      "Bot said: today?\n",
      "I'm doing well, thanks for asking. It's a beautiful day outside, and I\n",
      "Say something...\n",
      "User said: and you\n",
      "Bot said: ? \n",
      "\n",
      "P.S. I hope you're not too busy to chat. I'd love to hear\n",
      "Say something...\n",
      "User said: what do you want to\n",
      "Bot said: talk about?\n",
      "\n",
      "## Step 1: Identify the greeting and response format\n",
      "The greeting is a friendly\n",
      "Say something...\n",
      "User said: goodbye\n",
      "Exiting chatbot loop.\n"
     ]
    }
   ],
   "source": [
    "conversations = \"\"\n",
    "while True:\n",
    "    prompt = listen(phrase_time_limit=10)\n",
    "    if not prompt:\n",
    "        say(\"No input detected, please try again.\", rate=150)\n",
    "        continue\n",
    "    print(f\"User said: {prompt}\")\n",
    "    if prompt.lower().strip() in [\"exit\", \"quit\", \"goodbye\", \"bye\", \"good bye\", \"cancel\"]:\n",
    "        say(\"Good bye!\", rate=150)\n",
    "        print(\"Exiting chatbot loop.\")\n",
    "        break\n",
    "    conversations += f\"{prompt}\"\n",
    "    response = chat(model, torch.device(\"cuda\"), conversations, max_new_tokens=20, silent=True)\n",
    "    response = response[len(conversations):].strip()\n",
    "    print(f\"Bot said: {response}\")\n",
    "    conversations += f\"\\n{response}.\"\n",
    "    say(response, rate=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5a7aaa6",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
