{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4022d795-eeef-4a79-b3e7-52f17c5b688e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.12.7\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c53ece2a-c5ac-492a-bb57-82f4643e99ea",
   "metadata": {},
   "source": [
    "Getting the model\n",
    "---------------------------------\n",
    "\n",
    "pip install llama-stack\n",
    "\n",
    "llama model list\n",
    "\n",
    "llama model download --source meta --model-id Llama3.2-1B\n",
    "\n",
    "https://www.llama.com/llama-downloads/\n",
    "\n",
    "1B-3B:\n",
    "https://llama3-2-lightweight.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiandpMzNzOGZmMTR0cGd1MzIzenNicGg5IiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTItbGlnaHR3ZWlnaHQubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjU0NDQxOH19fV19&Signature=CtE31IHKvBy1dMf6bZkPn%7E5GZMimy-DYhSnYXqm3weFP1e19w2pWeGWDCgN4L6dWpxW0r8zoUaGimB0if8YzIoviHPjve4q4pXJWxFadO2vEwF0hVNGqVWtIAXjfMH72EK7WAh62p5LFLkvKMIdMM4nVGiomXfGIkPOU8dpWOMoq3XqoQakQyZjvxwdvodYBPzfP-tSEkTAhbTG9uunpkG1LAKQ-AhYDLESssGzdTzzZGyhk8GeiB11-Sgj9xS6KQlG94iZ%7ETSYHh8M8f6f3cEVtJ7gsjeILrnHZ54NUu24C%7EjJstLsxHxb6Ym1dG2Lu8WKguZpSZVxp6QZvubPcMg__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=6972155746241380\n",
    "\n",
    "11B-90B:\n",
    "https://llama3-2-multimodal.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiMnJ2dDI1ZDk5azV4YzhuZXBqeGJlZ3NkIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTItbXVsdGltb2RhbC5sbGFtYW1ldGEubmV0XC8qIiwiQ29uZGl0aW9uIjp7IkRhdGVMZXNzVGhhbiI6eyJBV1M6RXBvY2hUaW1lIjoxNzMyNTkxMjc0fX19XX0_&Signature=YgYg4sR3YGmSla-nag63tSykccKe60HGTT2eESHu1Tcdb3Vc0JnrdRgFbn40g%7EmVCtAcyghUSoKo6N2IXPzNsS8LhcP0IlFyNBISRL02tl64EXxRjQkeNI8-r8BN%7EC0LXeX3j-OR9lhLa4V1ut8FrzBVGO9hL%7EM3KT%7EylI0HPB3F7roTLj6Zbj4MAm7okHu-tmwxOwdRpHLsC23xxPSLxQb2VI2wKhxStWYTJ4W-YmpCRXUfwv4YkKLHwhFiLBB9nE3PnFPYNrNzLj2KdcNmTiwlaaPgyevIXK1e1zO1XQhFmOF811B1PlLQZeyEUszvKRVDA3fVUoSPwTTfIaDXpw__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1571579543745135\n",
    "\n",
    "40B-70B & 8B:\n",
    "https://llama3-1.llamameta.net/*?Policy=eyJTdGF0ZW1lbnQiOlt7InVuaXF1ZV9oYXNoIjoiNW44ZzI3N2FlaXUxZXRnZmlsZWszb3ZhIiwiUmVzb3VyY2UiOiJodHRwczpcL1wvbGxhbWEzLTEubGxhbWFtZXRhLm5ldFwvKiIsIkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTczMjU5MTI3NH19fV19&Signature=kIPDlKjVQtvkG9zmMeyPkrighTK49BfdeR3Ir2ZB8ZQe02XPw3lK0o-m4Y2Q54oKy8CrpKvmAvUZZKEG3UxRFJ%7EgFi-zzf-E1AajjVLqh1bfB9oHz5rTBG5logo1IZlDajDgD8B9RhNWQjmPGtDfrePjA0RFQGCvqjbwXg4ApRYjgsAnN5YqrwQGsXwXdhB6IfTz-muvnqu2zxJ32CwsUZV6Auhgg0J9sYuZSi420RmYOu31fLYXhUpbqxJdjFInkPI%7EBZrj8Wb4bPE4OAEatjHeogN8yaV-TmPtNCpZJYvkF50j84P1tKvi-psBhocti-76goXxEfZkMqAAurrMHQ__&Key-Pair-Id=K15QRJLYKIFSLZ&Download-Request-ID=1749650625811381"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f624fa44-e404-48a8-aad0-01e68f8fc003",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m pip install --upgrade pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9741c248-6c29-445a-aa79-daa1a7a29a1f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# !pip install torch fairscale fire blobfile transformers sentencepiece protobuf\n",
    "# !pip install --upgrade pip setuptools wheel\n",
    "# !pip install git+https://github.com/meta-llama/llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "94810639-d91d-4875-b127-f663d224d9f2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['checklist.chk', 'consolidated.00.pth', 'params.json', 'tokenizer.model']\n"
     ]
    }
   ],
   "source": [
    "# Verify files\n",
    "# expected: ['checklist.chk', 'consolidated.00.pth', 'params.json', 'tokenizer.model']\n",
    "\n",
    "import os\n",
    "\n",
    "model_name = r\"Llama3.2-1B\"\n",
    "\n",
    "models_path = r\"./llama-models/\"\n",
    "model_path = models_path + model_name\n",
    "files = os.listdir(model_path)\n",
    "print(files)\n",
    "\n",
    "checkpoint_file = model_path + r\"/consolidated.00.pth\"\n",
    "params_file = model_path + r\"/params.json\"\n",
    "tokenizer_file = model_path + r\"/tokenizer.model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "65224fa5-b6bd-49bf-8b8d-92b68136bf7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully from checkpoint.\n"
     ]
    }
   ],
   "source": [
    "# load model from consolidated.00.pth\n",
    "\n",
    "import torch\n",
    "\n",
    "state_dict = None\n",
    "try:\n",
    "    state_dict = torch.load(checkpoint_file, map_location=\"cpu\", weights_only=False)\n",
    "    print(\"Model loaded successfully from checkpoint.\")\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError while loading model checkpoint: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while loading model checkpoint: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ebf7c316-6207-4e1a-aa4a-bcf0eadcae6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Configuration loaded successfully from params.json.\n",
      "{\n",
      "  \"dim\": 2048,\n",
      "  \"ffn_dim_multiplier\": 1.5,\n",
      "  \"multiple_of\": 256,\n",
      "  \"n_heads\": 32,\n",
      "  \"n_kv_heads\": 8,\n",
      "  \"n_layers\": 16,\n",
      "  \"norm_eps\": 1e-05,\n",
      "  \"rope_theta\": 500000.0,\n",
      "  \"use_scaled_rope\": true,\n",
      "  \"vocab_size\": 128256\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Load config from params.json\n",
    "\n",
    "import json\n",
    "\n",
    "config = None\n",
    "try:\n",
    "    with open(params_file, 'r') as f:\n",
    "        config = json.load(f)\n",
    "    print(\"Configuration loaded successfully from params.json.\")\n",
    "    print(json.dumps(config, indent=2))\n",
    "except PermissionError as e:\n",
    "    print(f\"PermissionError while loading config: {e}\")\n",
    "except Exception as e:\n",
    "    print(f\"Error while loading config: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1be3084a-7b3e-4e26-bb94-781cc12ba64b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Manually reading tokenizer.model\n",
    "\n",
    "# import base64\n",
    "\n",
    "# def load_tokenizer_model(file_path):\n",
    "#     token_vocab = {}\n",
    "#     with open(tokenizer_file, 'r') as file:\n",
    "#         for line in file:\n",
    "#             token, token_id = line.split(' ')\n",
    "#             token = base64.b64decode(token).decode('utf-8', errors='ignore')\n",
    "#             token_id = int(token_id.strip())\n",
    "#             token_vocab[token] = token_id\n",
    "#     return token_vocab\n",
    "\n",
    "# def tokenize(text, token_vocab):\n",
    "#     tokens = []\n",
    "#     word = text.strip()\n",
    "#     while word:\n",
    "#         matched = False\n",
    "#         for i in range(len(word), 0, -1):\n",
    "#             subword = word[:i]\n",
    "#             if subword in token_vocab:\n",
    "#                 tokens.append(subword)\n",
    "#                 word = word[i:]\n",
    "#                 matched = True\n",
    "#                 break\n",
    "#         if not matched:\n",
    "#             print(\"Unknown token part:\", word)\n",
    "#             break\n",
    "#     return tokens\n",
    "\n",
    "# def detokenize(tokens):\n",
    "#     return \"\".join(tokens)\n",
    "\n",
    "# token_vocab = load_tokenizer_model(tokenizer_file)\n",
    "# text = \"hello world\"\n",
    "# tokens = tokenize(text, token_vocab)\n",
    "# print(\"Tokens:\", tokens)\n",
    "# detokenized_text = detokenize(tokens)\n",
    "# print(\"Detokenized text:\", detokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e8be6441-f4e4-45a9-9332-8db89b4b0c5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import torch\n",
    "# import json\n",
    "# from torch import nn\n",
    "\n",
    "# # Define a simple transformer model (just a placeholder - you'd need the actual LLaMA model architecture here)\n",
    "# class SimpleTransformerModel(nn.Module):\n",
    "#     def __init__(self, config):\n",
    "#         super(SimpleTransformerModel, self).__init__()\n",
    "#         # Use the correct keys based on the config\n",
    "#         self.num_layers = config.get(\"n_layers\", 12)  # Default to 12 if key is missing\n",
    "#         self.hidden_size = config.get(\"hidden_size\", 768)  # Default to 768 if key is missing\n",
    "#         self.vocab_size = config.get(\"vocab_size\", 30522)  # Default to 30522 if key is missing\n",
    "#         self.model = nn.Transformer(d_model=self.hidden_size, num_encoder_layers=self.num_layers)\n",
    "    \n",
    "#     def forward(self, x):\n",
    "#         return self.model(x)\n",
    "\n",
    "\n",
    "# # Initialize the model with the loaded config\n",
    "# model = SimpleTransformerModel(config)\n",
    "\n",
    "# # Load the model weights\n",
    "# model.load_state_dict(state_dict, strict=False)\n",
    "# print(\"Simple Model weights loaded successfully.\")\n",
    "\n",
    "# # Move the model to the appropriate device (GPU if available)\n",
    "# device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "# model.to(device)\n",
    "\n",
    "# model.eval()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea05509-c15b-4b38-8a9f-82242ea5d042",
   "metadata": {},
   "source": [
    "Transforming the model\n",
    "-----------------------------------\n",
    "\n",
    "https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/convert_llama_weights_to_hf.py\n",
    "\n",
    "https://huggingface.co/docs/transformers/main/model_doc/llama2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "912c8e49-a000-4b04-99b9-80fd149ddb39",
   "metadata": {},
   "source": [
    "pip install accelerate>=0.26.0\n",
    "\n",
    "cd D:\\Projects\\my-jupyter-notebook\n",
    "\n",
    "mv \"C:\\Users\\User\\C-\\Users\\User\\.llama\\checkpoints\\Llama3.2-1B\" \"llama-models\\\"\n",
    "\n",
    "python llama-models/convert_llama_weights_to_hf.py --input_dir llama-models/Llama3.2-1B --model_size 1B --llama_version 3.2 --output_dir llama-models/Llama3.2-1B-transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4803bb20-c4e8-4527-b3fb-54e8612ee6ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json']\n"
     ]
    }
   ],
   "source": [
    "# Verify transformed files\n",
    "# expected: ['config.json', 'generation_config.json', 'model.safetensors', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json']\n",
    "\n",
    "transformed_model_path = models_path + model_name + r\"-transformed\"\n",
    "files = os.listdir(transformed_model_path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42f62dfa-4af3-4e68-822d-b004fcab6c20",
   "metadata": {},
   "source": [
    "Use Model\n",
    "----------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9c6d4c20-c446-4853-a2b7-6dda9b8423e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaForCausalLM\n",
    "model = LlamaForCausalLM.from_pretrained(transformed_model_path)\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "367e4e61-51be-460c-826a-e044fd829ccf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import LlamaTokenizer, AutoTokenizer, PreTrainedTokenizerFast\n",
    "# tokenizer = LlamaTokenizer.from_pretrained(transformed_model_path)\n",
    "# tokenizer = PreTrainedTokenizerFast(transformed_model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(transformed_model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e20ef748-07b9-4556-8dce-b7e1a9806ec1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me an interesting fact about elephants.\n",
      "Elephants are the largest land animals. They are also the most intelligent. They are known to communicate with each other by grunting, and they can also communicate with humans by using signs and gestures. They are also known for their ability to recognize and remember faces.\n",
      "What are the differences between elephants and humans?\n",
      "Elephants are the largest land animals, and humans are the largest primates. They have a very different body structure and physiology. Elephants have\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "# inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "generate_ids = model.generate(inputs.input_ids, max_length=100, eos_token_id=None)\n",
    "response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "print(response[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1b7a6a9-eda7-4e8b-b4b3-db757cb5464f",
   "metadata": {},
   "source": [
    "CUDA vs CPU\n",
    "-----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d7b55774-6a25-4fba-be17-372b65950c09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cuda available: True\n",
      "Cuda device count: 1\n",
      "Device Name: NVIDIA GeForce RTX 3080 Laptop GPU\n",
      "Total Memory (GB): 17.179344896\n",
      "Core count: 48\n",
      "CUDA Capability (Compute Capability): 8.6\n",
      "PyTorch version: 2.5.1+cu118\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "print(f\"Cuda available: {torch.cuda.is_available()}\")\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"Cuda device count: {torch.cuda.device_count()}\")\n",
    "    device_properties = torch.cuda.get_device_properties(device)\n",
    "    print(f\"Device Name: {device_properties.name}\")\n",
    "    print(f\"Total Memory (GB): {device_properties.total_memory / 1e9}\")\n",
    "    print(f\"Core count: {device_properties.multi_processor_count}\")\n",
    "    print(f\"CUDA Capability (Compute Capability): {device_properties.major}.{device_properties.minor}\")\n",
    "    print(f\"PyTorch version: {torch.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3910ceb-a9ef-4d65-93bd-5fea158ebf6b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me an interesting fact about elephants\n",
      "I have a special place in my heart for elephants. I love their gentle nature and how they are so loyal and protective of their families. They are truly amazing animals and I feel so lucky to have seen them up close.\n",
      "I have a special place in my heart for elephants. I love their gentle nature and how they are so loyal and protective of their families. They are truly amazing animals and I feel so lucky to have seen them up close.\n",
      "The\n",
      "\n",
      "(14.4398 seconds taken on cpu)\n",
      "---------------------\n",
      "\n",
      "Tell me an interesting fact about elephants.\n",
      "I love elephants. I think they are the most intelligent animals on earth. Elephants are also the most social animals. They are very good at working together. Elephants are also very good at making friends. Elephants can also be very bad. Elephants can be very aggressive. Elephants can also be very dangerous. Elephants can also be very slow. Elephants can also be very slow. Elephants can also be very slow. Elephants\n",
      "\n",
      "(2.7015 seconds taken on cuda:0)\n",
      "---------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import torch, time\n",
    "\n",
    "def chat(model, device, prompt):\n",
    "    start_time = time.time()\n",
    "    model = model.to(device)\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n",
    "    generate_ids = model.generate(inputs.input_ids, max_length=100, eos_token_id=None)\n",
    "    response = tokenizer.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)\n",
    "    print(f\"{response[0]}\")\n",
    "    elapsed_time = time.time() - start_time\n",
    "    print(f\"\\n({elapsed_time:.4f} seconds taken on {model.device})\\n---------------------\\n\")\n",
    "\n",
    "prompt = \"Tell me an interesting fact about elephants\"\n",
    "chat(model, torch.device(\"cpu\"), prompt)\n",
    "chat(model, torch.device(\"cuda\"), prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dd1d020-e90e-4f5d-a995-a5e73985645b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6c3e673-1ac9-44b3-91d9-8d809fcfc90e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
